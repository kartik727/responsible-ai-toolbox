{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from ml_wrappers.model import OpenaiWrapperModel\n",
    "from transformers import pipeline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'title', 'context', 'question', 'answers'],\n",
       "    num_rows: 87599\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.load_dataset(\"squad\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "context = []\n",
    "answers = []\n",
    "prompts = []\n",
    "template = 'Answer the question given the context.\\n\\ncontext:\\n{context}\\n\\nquestion:\\n{question}'\n",
    "for row in dataset:\n",
    "    context.append(row['context'])\n",
    "    questions.append(row['question'])\n",
    "    answers.append(row['answers']['text'][0])\n",
    "    templated_prompt = template.format(context=row['context'], question=row['question'])\n",
    "    prompts.append(templated_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Answer the question given the context.\\n\\ncont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Answer the question given the context.\\n\\ncont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Answer the question given the context.\\n\\ncont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Answer the question given the context.\\n\\ncont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Answer the question given the context.\\n\\ncont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt\n",
       "0  Answer the question given the context.\\n\\ncont...\n",
       "1  Answer the question given the context.\\n\\ncont...\n",
       "2  Answer the question given the context.\\n\\ncont...\n",
       "3  Answer the question given the context.\\n\\ncont...\n",
       "4  Answer the question given the context.\\n\\ncont..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame({\n",
    "    # 'context': context,\n",
    "    # 'questions': questions,\n",
    "    # 'answers': answers,\n",
    "    'prompt' : prompts})\n",
    "test_data = data[:3]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class template(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def predict(self, dataset):\n",
    "        dummy = 'This is a dummy answer'\n",
    "        return np.array([dummy for _ in range(len(dataset))])\n",
    "        # template = 'Answer the question given the context.'\n",
    "        # for i, (context, question) in enumerate(zip(dataset['context'], dataset['questions'])):\n",
    "        #     templated_question = template + '\\n\\ncontext: ' + context + '\\nquestion: ' + question\n",
    "        #     if isinstance(dataset, pd.DataFrame):\n",
    "        #         dataset.iloc[i]['questions'] = templated_question\n",
    "        #     else:\n",
    "        #         dataset['questions'] = templated_question\n",
    "        # return self.model.predict(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_model = OpenaiWrapperModel(\n",
    "    os.getenv('OPENAI_API_TYPE'),\n",
    "    os.getenv('OPENAI_API_ENDPOINT'),\n",
    "    os.getenv('OPENAI_API_VERSION'),\n",
    "    os.getenv('OPENAI_API_KEY'),\n",
    "    engine='gpt-4')\n",
    "\n",
    "pipeline_model = template(openai_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model = OpenaiWrapperModel(\n",
    "    os.getenv('OPENAI_API_TYPE'),\n",
    "    os.getenv('OPENAI_API_ENDPOINT'),\n",
    "    os.getenv('OPENAI_API_VERSION'),\n",
    "    os.getenv('OPENAI_API_KEY'),\n",
    "    engine='gpt-4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAI Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset download attempt 1 of 4\n"
     ]
    }
   ],
   "source": [
    "from responsibleai_text import RAITextInsights, ModelTask\n",
    "from raiwidgets import ResponsibleAIDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Answer the question given the context.\\n\\ncont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Answer the question given the context.\\n\\ncont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Answer the question given the context.\\n\\ncont...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt\n",
       "0  Answer the question given the context.\\n\\ncont...\n",
       "1  Answer the question given the context.\\n\\ncont...\n",
       "2  Answer the question given the context.\\n\\ncont..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature extraction: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "feature extraction: 3it [00:00,  3.04it/s]\n",
      "Failed to parse metric `This is a dummy answer`: invalid literal for int() with base 10: 'This is a dummy answer'\n",
      "Failed to parse metric `This is a dummy answer`: invalid literal for int() with base 10: 'This is a dummy answer'\n",
      "Failed to parse metric `This is a dummy answer`: invalid literal for int() with base 10: 'This is a dummy answer'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing coherence score\n",
      "coherence score\n",
      "[0, 0, 0]\n",
      "ext_dataset\n",
      "['positive_words', 'negative_words', 'negation_words', 'negated_entities', 'named_persons', 'sentence_length', 'target_score']\n",
      "   positive_words  negative_words  negation_words  negated_entities  \\\n",
      "0              50               0               0                 0   \n",
      "1              50               0               0                 0   \n",
      "2              52               0               0                 0   \n",
      "\n",
      "   named_persons  sentence_length  target_score  \n",
      "0              3              827             5  \n",
      "1              2              805             5  \n",
      "2              3              832             5  \n"
     ]
    }
   ],
   "source": [
    "rai_insights = RAITextInsights(\n",
    "    pipeline_model, test_data, None,\n",
    "    task_type=ModelTask.GENERATIVE_TEXT,\n",
    "    text_column='prompt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Error Analysis\n",
      "Current Status: Generating error analysis reports.\n",
      "Current Status: Finished generating error analysis reports.\n",
      "Time taken: 0.0 min 0.3656380000002173 sec\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "rai_insights.error_analysis.add()\n",
    "rai_insights.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResponsibleAI started at http://localhost:8704\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<raiwidgets.responsibleai_dashboard.ResponsibleAIDashboard at 0x2858abb3e20>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ResponsibleAIDashboard(rai_insights)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
